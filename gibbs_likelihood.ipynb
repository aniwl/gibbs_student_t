{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do not have mpi4py package.\n",
      "Do not have acor package\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import os, sys, time\n",
    "\n",
    "import numpy as np\n",
    "import scipy.linalg as sl, scipy.stats, scipy.special\n",
    "import matplotlib.pyplot as plt\n",
    "import libstempo as t2\n",
    "\n",
    "from enterprise.pulsar import Pulsar\n",
    "import enterprise.constants as const\n",
    "from enterprise.signals import parameter\n",
    "from enterprise.signals import utils\n",
    "from enterprise.signals import prior\n",
    "from enterprise.signals import selections\n",
    "from enterprise.signals import white_signals\n",
    "from enterprise.signals import gp_signals\n",
    "from enterprise.signals import signal_base\n",
    "\n",
    "from PTMCMCSampler.PTMCMCSampler import PTSampler as ptmcmc\n",
    "from corner import corner\n",
    "\n",
    "% matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Gibbs(object):\n",
    "    def __init__(self, pta, model='gaussian', tdf=4):\n",
    "        \n",
    "        self.pta = pta\n",
    "        \n",
    "        # For now assume one pulsar\n",
    "        self._residuals = self.pta.get_residuals()[0]\n",
    "        \n",
    "        # which likelihood model\n",
    "        self._lmodel = model\n",
    "        \n",
    "        # auxiliary variable stuff\n",
    "        self._b = np.zeros(self.pta.get_basis()[0].shape[1])\n",
    "        \n",
    "        \n",
    "        # outlier stuff\n",
    "        self._z = np.zeros_like(self._residuals)\n",
    "        self._alpha = np.ones_like(self._residuals)\n",
    "        self._theta = np.ones_like(self._residuals) * 0.01\n",
    "        self.tdf = tdf\n",
    "        if model in ['t', 'mixture']:\n",
    "            self._z = np.ones_like(self._residuals)        \n",
    "        \n",
    "    @property\n",
    "    def params(self):\n",
    "        ret = []\n",
    "        for param in self.pta.params:\n",
    "            ret.append(param)\n",
    "        return ret\n",
    "    \n",
    "    def map_params(self, xs):\n",
    "        return {par.name: x for par, x in zip(self.params, xs)}\n",
    "    \n",
    "    \n",
    "    def get_hyper_param_indices(self):\n",
    "        ind = []\n",
    "        for ct, par in enumerate(self.params):\n",
    "            if 'ecorr' in par.name or 'log10_A' in par.name or 'gamma' in par.name:\n",
    "                ind.append(ct)\n",
    "        return np.array(ind)  \n",
    "    \n",
    "    \n",
    "    def get_white_noise_indices(self):\n",
    "        ind = []\n",
    "        for ct, par in enumerate(self.params):\n",
    "            if 'efac' in par.name or 'equad' in par.name:\n",
    "                ind.append(ct)\n",
    "        return np.array(ind)  \n",
    "    \n",
    "    \n",
    "    def update_hyper_params(self, xs):\n",
    "        \n",
    "        # get hyper parameter indices\n",
    "        hind = self.get_hyper_param_indices()\n",
    "        \n",
    "        # get initial log-likelihood and log-prior\n",
    "        lnlike0, lnprior0 = self.get_lnlikelihood(xs), self.get_lnprior(xs)\n",
    "        xnew = xs.copy()\n",
    "        for ii in range(10):\n",
    "    \n",
    "            # standard gaussian jump (this allows for different step sizes)\n",
    "            q = xnew.copy()\n",
    "            sigmas = 0.05 * len(hind)\n",
    "            probs = [0.1, 0.15, 0.5, 0.15, 0.1]\n",
    "            sizes = [0.1, 0.5, 1.0, 3.0, 10.0]\n",
    "            scale = np.random.choice(sizes, p=probs)\n",
    "            par = np.random.choice(hind, size=1)\n",
    "            q[par] += np.random.randn(len(q[par])) * sigmas * scale\n",
    "\n",
    "            # get log-like and log prior at new position\n",
    "            lnlike1, lnprior1 = self.get_lnlikelihood(q), self.get_lnprior(q)\n",
    "\n",
    "            # metropolis step\n",
    "            diff = (lnlike1 + lnprior1) - (lnlike0 + lnprior0) \n",
    "            if diff > np.log(np.random.rand()):\n",
    "                xnew = q\n",
    "                lnlike0 = lnlike1\n",
    "                lnprior0 = lnprior1\n",
    "            else:\n",
    "                xnew = xnew\n",
    "\n",
    "        return xnew\n",
    "        \n",
    "        \n",
    "    def update_white_params(self, xs):\n",
    "        \n",
    "        # get white noise parameter indices\n",
    "        wind = self.get_white_noise_indices()\n",
    "        \n",
    "        xnew = xs.copy()\n",
    "        lnlike0, lnprior0 = self.get_lnlikelihood_white(xnew), self.get_lnprior(xnew)\n",
    "        for ii in range(20):\n",
    "        \n",
    "            # standard gaussian jump (this allows for different step sizes)\n",
    "            q = xnew.copy()\n",
    "            sigmas = 0.05 * len(wind)\n",
    "            probs = [0.1, 0.15, 0.5, 0.15, 0.1]\n",
    "            sizes = [0.1, 0.5, 1.0, 3.0, 10.0]\n",
    "            scale = np.random.choice(sizes, p=probs)\n",
    "            par = np.random.choice(wind, size=1)\n",
    "            q[par] += np.random.randn(len(q[par])) * sigmas * scale\n",
    "\n",
    "            # get log-like and log prior at new position\n",
    "            lnlike1, lnprior1 = self.get_lnlikelihood_white(q), self.get_lnprior(q)\n",
    "\n",
    "            # metropolis step\n",
    "            diff = (lnlike1 + lnprior1) - (lnlike0 + lnprior0) \n",
    "            if diff > np.log(np.random.rand()):\n",
    "                xnew = q\n",
    "                lnlike0 = lnlike1\n",
    "                lnprior0 = lnprior1\n",
    "            else:\n",
    "                xnew = xnew\n",
    "        return xnew\n",
    "        \n",
    "    def update_b(self, xs):\n",
    "        \n",
    "        # map parameter vector\n",
    "        params = self.map_params(xs)\n",
    "        \n",
    "        # start likelihood calculations\n",
    "        loglike = 0\n",
    "        \n",
    "        # get auxiliaries\n",
    "        Nvec = self._alpha**self._z * self.pta.get_ndiag(params)[0]\n",
    "        phiinv = self.pta.get_phiinv(params, logdet=False)[0]\n",
    "        residuals = self._residuals\n",
    "        \n",
    "        T = self.pta.get_basis(params)[0]\n",
    "        TNT = np.dot(T.T, T / Nvec[:,None])\n",
    "        d = np.dot(T.T, residuals/Nvec)\n",
    "\n",
    "        #d = self.pta.get_TNr(params)[0]\n",
    "        #TNT = self.pta.get_TNT(params)[0]\n",
    "\n",
    "        # Red noise piece\n",
    "        Sigma = TNT + np.diag(phiinv)\n",
    "\n",
    "        try:\n",
    "            u, s, _ = sl.svd(Sigma)\n",
    "            mn = np.dot(u, np.dot(u.T, d)/s)\n",
    "            Li = u * np.sqrt(1/s)\n",
    "        except np.linalg.LinAlgError:\n",
    "\n",
    "            Q, R = sl.qr(Sigma)\n",
    "            Sigi = sl.solve(R, Q.T)\n",
    "            mn = np.dot(Sigi, d)\n",
    "            u, s, _ = sl.svd(Sigi)\n",
    "            Li = u * np.sqrt(1/s)\n",
    "\n",
    "        b = mn + np.dot(Li, np.random.randn(Li.shape[0]))\n",
    "        \n",
    "        return b\n",
    "    \n",
    "    \n",
    "    def update_theta(self, xs):\n",
    "        \n",
    "        if self._lmodel in ['t', 'gaussian']:\n",
    "            return self._theta\n",
    "        elif self._lmodel == 'mixture':\n",
    "            return scipy.stats.beta.rvs(self._z+1, 2-self._z)\n",
    "        \n",
    "        \n",
    "    def update_z(self, xs):\n",
    "        \n",
    "        # map parameters \n",
    "        params = self.map_params(xs)\n",
    "        \n",
    "        if self._lmodel in ['t', 'gaussian']:\n",
    "            return self._z\n",
    "        elif self._lmodel == 'mixture':\n",
    "            Nvec0 = self.pta.get_ndiag(params)[0]\n",
    "            Nvec = self._alpha**self._z * Nvec0\n",
    "            \n",
    "            Tmat = self.pta.get_basis(params)[0]\n",
    "            theta_mean = np.dot(Tmat, self._b)\n",
    "            top = self._theta * scipy.stats.norm.pdf(self._residuals, loc=theta_mean, \n",
    "                                                     scale=np.sqrt(self._alpha*Nvec0))\n",
    "            bot = top + (1-self._theta) * scipy.stats.norm.pdf(self._residuals,\n",
    "                                                               loc=theta_mean, \n",
    "                                                               scale=np.sqrt(Nvec0))\n",
    "            q = top / bot\n",
    "            q[np.isnan(q)] = 1\n",
    "            return scipy.stats.binom.rvs(1, map(lambda x: min(x, 1), q))\n",
    "        \n",
    "        \n",
    "    def update_alpha(self, xs):\n",
    "        \n",
    "        # map parameters \n",
    "        params = self.map_params(xs)\n",
    "        \n",
    "        if np.sum(self._z) >= 1:\n",
    "            Nvec0 = self.pta.get_ndiag(params)[0]\n",
    "            Tmat = self.pta.get_basis(params)[0]\n",
    "            theta_mean = np.dot(Tmat, self._b)\n",
    "            top = ((self._residuals-theta_mean)**2 * self._z / Nvec0 + self.tdf) / 2\n",
    "            bot = scipy.stats.gamma.rvs((self._z+self.tdf)/2)\n",
    "            return top / bot\n",
    "        else:\n",
    "            return self._alpha\n",
    "    \n",
    "        \n",
    "    def get_lnlikelihood_white(self, xs):\n",
    "        \n",
    "        # map parameters \n",
    "        params = self.map_params(xs)\n",
    "        \n",
    "        # Nvec and Tmat\n",
    "        Nvec = self._alpha**self._z * self.pta.get_ndiag(params)[0]\n",
    "        Tmat = self.pta.get_basis(params)[0]\n",
    "        \n",
    "        # whitened residuals\n",
    "        yred = self._residuals - np.dot(Tmat, self._b)\n",
    "        \n",
    "        # log determinant of N\n",
    "        logdet_N = np.sum(np.log(Nvec))\n",
    "\n",
    "        # triple product in likelihood function\n",
    "        rNr = np.sum(yred**2/Nvec)\n",
    "\n",
    "        # first component of likelihood function\n",
    "        loglike = -0.5 * (logdet_N + rNr)\n",
    "            \n",
    "        return loglike   \n",
    "    \n",
    "\n",
    "    # this can and should be much cleaner\n",
    "    def get_lnlikelihood(self, xs):\n",
    "        \n",
    "        # map parameter vector\n",
    "        params = self.map_params(xs)\n",
    "        \n",
    "        # start likelihood calculations\n",
    "        loglike = 0\n",
    "        \n",
    "        # get auxiliaries\n",
    "        Nvec = self._alpha**self._z * self.pta.get_ndiag(params)[0]\n",
    "        phiinv, logdet_phi = self.pta.get_phiinv(params, logdet=True)[0]\n",
    "        residuals = self._residuals\n",
    "        \n",
    "        T = self.pta.get_basis(params)[0]\n",
    "        TNT = np.dot(T.T, T / Nvec[:,None])\n",
    "        d = np.dot(T.T, residuals/Nvec)\n",
    "\n",
    "        #d = self.pta.get_TNr(params)[0]\n",
    "        #TNT = self.pta.get_TNT(params)[0]\n",
    "\n",
    "        # log determinant of N\n",
    "        logdet_N = np.sum(np.log(Nvec))\n",
    "\n",
    "        # triple product in likelihood function\n",
    "        rNr = np.sum(residuals**2/Nvec)\n",
    "\n",
    "        # first component of likelihood function\n",
    "        loglike += -0.5 * (logdet_N + rNr)\n",
    "\n",
    "        # Red noise piece\n",
    "        Sigma = TNT + np.diag(phiinv)\n",
    "        \n",
    "        try:\n",
    "            cf = sl.cho_factor(Sigma)\n",
    "            expval = sl.cho_solve(cf, d)\n",
    "        except np.linalg.LinAlgError:\n",
    "            return -np.inf \n",
    "        \n",
    "        logdet_sigma = np.sum(2 * np.log(np.diag(cf[0])))\n",
    "        loglike += 0.5 * (np.dot(d, expval) - logdet_sigma - logdet_phi)\n",
    "        \n",
    "        return loglike\n",
    "    \n",
    "    \n",
    "    def get_lnprior(self, xs):\n",
    "        \n",
    "        return np.sum(p.get_logpdf(x) for p, x in zip(self.params, xs))\n",
    "    \n",
    "    \n",
    "    def sample(self, xs, niter=10000):\n",
    "        \n",
    "        self.chain = np.zeros((niter, len(xs)))\n",
    "        self.bchain = np.zeros((niter, len(self._b)))\n",
    "        self.thetachain = np.zeros((niter, len(self._residuals)))\n",
    "        self.zchain = np.zeros((niter, len(self._residuals)))\n",
    "        self.alphachain = np.zeros((niter, len(self._residuals)))               \n",
    "        \n",
    "        xnew = xs\n",
    "        tstart = time.time()\n",
    "        for ii in range(niter):\n",
    "            self.chain[ii, :] = xnew\n",
    "            self.bchain[ii,:] = self._b\n",
    "            self.zchain[ii,:] = self._z\n",
    "            self.thetachain[ii,:] = self._theta\n",
    "            self.alphachain[ii,:] = self._alpha\n",
    "\n",
    "            # update white parameters\n",
    "            xnew = self.update_white_params(xnew)\n",
    "            \n",
    "            # update hyper-parameters\n",
    "            xnew = self.update_hyper_params(xnew)\n",
    "            \n",
    "            # if accepted update quadratic params\n",
    "            if np.all(xnew != self.chain[ii,-1]):\n",
    "                self._b = self.update_b(xnew)\n",
    "            \n",
    "            # update outlier model params\n",
    "            self._theta = self.update_theta(xnew)\n",
    "            self._z = self.update_z(xnew)\n",
    "            self._alpha = self.update_alpha(xnew)\n",
    "            \n",
    "            if ii % 100 == 0 and ii > 0:\n",
    "                sys.stdout.write('\\r')\n",
    "                sys.stdout.write('Finished %g percent in %g seconds.'%(ii / niter * 100, time.time()-tstart))\n",
    "                sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Gibbs2(object):\n",
    "    def __init__(self, pta, model='gaussian', tdf=4, m=0.01, \n",
    "                 vary_df=True, theta_prior='beta', vary_alpha=True, alpha=1e10, pspin=None):\n",
    "        \n",
    "        self.pta = pta\n",
    "        \n",
    "        # a-prior outlier probability\n",
    "        self.mp = m\n",
    "        self.theta_prior = theta_prior\n",
    "        \n",
    "        # spin period\n",
    "        self.pspin = pspin\n",
    "        \n",
    "        # vary t-distribution d.o.f\n",
    "        self.vary_df = vary_df\n",
    "        \n",
    "        # vary alpha\n",
    "        self.vary_alpha = vary_alpha\n",
    "        \n",
    "        # For now assume one pulsar\n",
    "        self._residuals = self.pta.get_residuals()[0]\n",
    "        \n",
    "        # which likelihood model\n",
    "        self._lmodel = model\n",
    "        \n",
    "        # auxiliary variable stuff\n",
    "        self._b = np.zeros(self.pta.get_basis()[0].shape[1])\n",
    "        \n",
    "        # for caching\n",
    "        self.TNT = None\n",
    "        self.d = None\n",
    "        \n",
    "        # outlier stuff\n",
    "        self._pout = np.zeros_like(self._residuals)\n",
    "        self._z = np.zeros_like(self._residuals)\n",
    "        if not vary_alpha:\n",
    "            self._alpha = np.ones_like(self._residuals) * alpha\n",
    "        else:\n",
    "            self._alpha = np.ones_like(self._residuals) \n",
    "        self._theta = self.mp\n",
    "        self.tdf = tdf\n",
    "        if model in ['t', 'mixture', 'vvh17']:\n",
    "            self._z = np.ones_like(self._residuals)        \n",
    "        \n",
    "    @property\n",
    "    def params(self):\n",
    "        ret = []\n",
    "        for param in self.pta.params:\n",
    "            ret.append(param)\n",
    "        return ret\n",
    "    \n",
    "    def map_params(self, xs):\n",
    "        return {par.name: x for par, x in zip(self.params, xs)}\n",
    "    \n",
    "    \n",
    "    def get_hyper_param_indices(self):\n",
    "        ind = []\n",
    "        for ct, par in enumerate(self.params):\n",
    "            if 'ecorr' in par.name or 'log10_A' in par.name or 'gamma' in par.name:\n",
    "                ind.append(ct)\n",
    "        return np.array(ind)  \n",
    "    \n",
    "    \n",
    "    def get_white_noise_indices(self):\n",
    "        ind = []\n",
    "        for ct, par in enumerate(self.params):\n",
    "            if 'efac' in par.name or 'equad' in par.name:\n",
    "                ind.append(ct)\n",
    "        return np.array(ind)  \n",
    "    \n",
    "    \n",
    "    def update_hyper_params(self, xs):\n",
    "        \n",
    "        # get hyper parameter indices\n",
    "        hind = self.get_hyper_param_indices()\n",
    "        \n",
    "        # get initial log-likelihood and log-prior\n",
    "        lnlike0, lnprior0 = self.get_lnlikelihood(xs), self.get_lnprior(xs)\n",
    "        xnew = xs.copy()\n",
    "        for ii in range(10):\n",
    "    \n",
    "            # standard gaussian jump (this allows for different step sizes)\n",
    "            q = xnew.copy()\n",
    "            sigmas = 0.05 * len(hind)\n",
    "            probs = [0.1, 0.15, 0.5, 0.15, 0.1]\n",
    "            sizes = [0.1, 0.5, 1.0, 3.0, 10.0]\n",
    "            scale = np.random.choice(sizes, p=probs)\n",
    "            par = np.random.choice(hind, size=1)\n",
    "            q[par] += np.random.randn(len(q[par])) * sigmas * scale\n",
    "\n",
    "            # get log-like and log prior at new position\n",
    "            lnlike1, lnprior1 = self.get_lnlikelihood(q), self.get_lnprior(q)\n",
    "\n",
    "            # metropolis step\n",
    "            diff = (lnlike1 + lnprior1) - (lnlike0 + lnprior0) \n",
    "            if diff > np.log(np.random.rand()):\n",
    "                xnew = q\n",
    "                lnlike0 = lnlike1\n",
    "                lnprior0 = lnprior1\n",
    "            else:\n",
    "                xnew = xnew\n",
    "\n",
    "        return xnew\n",
    "        \n",
    "        \n",
    "    def update_white_params(self, xs):\n",
    "        \n",
    "        # get white noise parameter indices\n",
    "        wind = self.get_white_noise_indices()\n",
    "        \n",
    "        xnew = xs.copy()\n",
    "        lnlike0, lnprior0 = self.get_lnlikelihood_white(xnew), self.get_lnprior(xnew)\n",
    "        for ii in range(20):\n",
    "        \n",
    "            # standard gaussian jump (this allows for different step sizes)\n",
    "            q = xnew.copy()\n",
    "            sigmas = 0.05 * len(wind)\n",
    "            probs = [0.1, 0.15, 0.5, 0.15, 0.1]\n",
    "            sizes = [0.1, 0.5, 1.0, 3.0, 10.0]\n",
    "            scale = np.random.choice(sizes, p=probs)\n",
    "            par = np.random.choice(wind, size=1)\n",
    "            q[par] += np.random.randn(len(q[par])) * sigmas * scale\n",
    "\n",
    "            # get log-like and log prior at new position\n",
    "            lnlike1, lnprior1 = self.get_lnlikelihood_white(q), self.get_lnprior(q)\n",
    "\n",
    "            # metropolis step\n",
    "            diff = (lnlike1 + lnprior1) - (lnlike0 + lnprior0) \n",
    "            if diff > np.log(np.random.rand()):\n",
    "                xnew = q\n",
    "                lnlike0 = lnlike1\n",
    "                lnprior0 = lnprior1\n",
    "            else:\n",
    "                xnew = xnew\n",
    "        return xnew\n",
    "        \n",
    "    def update_b(self, xs):\n",
    "        \n",
    "        # map parameter vector\n",
    "        params = self.map_params(xs)\n",
    "        \n",
    "        # start likelihood calculations\n",
    "        loglike = 0\n",
    "        \n",
    "        # get auxiliaries\n",
    "        Nvec = self._alpha**self._z * self.pta.get_ndiag(params)[0]\n",
    "        phiinv = self.pta.get_phiinv(params, logdet=False)[0]\n",
    "        residuals = self._residuals\n",
    "        \n",
    "        T = self.pta.get_basis(params)[0]\n",
    "        if self.TNT is None and self.d is None:\n",
    "            self.TNT = np.dot(T.T, T / Nvec[:,None])\n",
    "            self.d = np.dot(T.T, residuals/Nvec)\n",
    "        #d = self.pta.get_TNr(params)[0]\n",
    "        #TNT = self.pta.get_TNT(params)[0]\n",
    "\n",
    "        # Red noise piece\n",
    "        Sigma = self.TNT + np.diag(phiinv)\n",
    "\n",
    "        try:\n",
    "            u, s, _ = sl.svd(Sigma)\n",
    "            mn = np.dot(u, np.dot(u.T, self.d)/s)\n",
    "            Li = u * np.sqrt(1/s)\n",
    "        except np.linalg.LinAlgError:\n",
    "\n",
    "            Q, R = sl.qr(Sigma)\n",
    "            Sigi = sl.solve(R, Q.T)\n",
    "            mn = np.dot(Sigi, self.d)\n",
    "            u, s, _ = sl.svd(Sigi)\n",
    "            Li = u * np.sqrt(1/s)\n",
    "\n",
    "        b = mn + np.dot(Li, np.random.randn(Li.shape[0]))\n",
    "        \n",
    "        return b\n",
    "    \n",
    "    \n",
    "    def update_theta(self, xs):\n",
    "        \n",
    "        if self._lmodel in ['t', 'gaussian']:\n",
    "            return self._theta\n",
    "        elif self._lmodel in ['mixture', 'vvh17']:\n",
    "            n = len(self._residuals)\n",
    "            if self.theta_prior == 'beta':\n",
    "                mk = n * self.mp\n",
    "                k1mm = n * (1-self.mp)\n",
    "            else:\n",
    "                mk, k1mm = 1.0, 1.0\n",
    "            ret = scipy.stats.beta.rvs(np.sum(self._z) + mk,\n",
    "                                       n - np.sum(self._z) + k1mm)\n",
    "            return ret\n",
    "        \n",
    "        \n",
    "    def update_z(self, xs):\n",
    "        \n",
    "        # map parameters \n",
    "        params = self.map_params(xs)\n",
    "        \n",
    "        if self._lmodel in ['t', 'gaussian']:\n",
    "            return self._z\n",
    "        elif self._lmodel in ['mixture', 'vvh17']:\n",
    "            Nvec0 = self.pta.get_ndiag(params)[0]\n",
    "            Tmat = self.pta.get_basis(params)[0]\n",
    "\n",
    "            Nvec = self._alpha * Nvec0\n",
    "            theta_mean = np.dot(Tmat, self._b)\n",
    "            top = self._theta * scipy.stats.norm.pdf(self._residuals, \n",
    "                                                     loc=theta_mean, \n",
    "                                                     scale=np.sqrt(Nvec))\n",
    "            if self._lmodel == 'vvh17':\n",
    "                top = self._theta / self.pspin\n",
    "\n",
    "            bot = top + (1-self._theta) * scipy.stats.norm.pdf(self._residuals,\n",
    "                                                               loc=theta_mean, \n",
    "                                                               scale=np.sqrt(Nvec0))\n",
    "            q = top / bot\n",
    "            q[np.isnan(q)] = 1\n",
    "            self._pout = q\n",
    "            return scipy.stats.binom.rvs(1, map(lambda x: min(x, 1), q))\n",
    "        \n",
    "        \n",
    "    def update_alpha(self, xs):\n",
    "        \n",
    "        # map parameters \n",
    "        params = self.map_params(xs)\n",
    "        \n",
    "        if np.sum(self._z) >= 1 and self.vary_alpha:\n",
    "            Nvec0 = self.pta.get_ndiag(params)[0]\n",
    "            Tmat = self.pta.get_basis(params)[0]\n",
    "            theta_mean = np.dot(Tmat, self._b)\n",
    "            top = ((self._residuals-theta_mean)**2 * self._z / Nvec0 + self.tdf) / 2\n",
    "            bot = scipy.stats.gamma.rvs((self._z+self.tdf)/2)\n",
    "            return top / bot\n",
    "        else:\n",
    "            return self._alpha\n",
    "    \n",
    "    def update_df(self, xs):\n",
    "        \n",
    "        if self.vary_df:\n",
    "            # 1. evaluate the log conditional posterior of df for 1, 2, ..., 30.\n",
    "            log_den_df = np.array(map(self.get_lnlikelihood_df, np.arange(1,31)))\n",
    "\n",
    "            # 2. normalize the probabilities\n",
    "            den_df = np.exp(log_den_df - log_den_df.max()) \n",
    "            den_df /= den_df.sum()\n",
    "\n",
    "            # 3. sample one of values (1, 2, ..., 30) according to the probabilities\n",
    "            df = np.random.choice(np.arange(1, 31), p=den_df)\n",
    "\n",
    "            return df\n",
    "        else:\n",
    "            return self.tdf\n",
    "\n",
    "        \n",
    "    def get_lnlikelihood_white(self, xs):\n",
    "        \n",
    "        # map parameters \n",
    "        params = self.map_params(xs)\n",
    "        \n",
    "        # Nvec and Tmat\n",
    "        Nvec = self._alpha**self._z * self.pta.get_ndiag(params)[0]\n",
    "        Tmat = self.pta.get_basis(params)[0]\n",
    "        \n",
    "        # whitened residuals\n",
    "        mn = np.dot(Tmat, self._b)\n",
    "        yred = self._residuals - mn\n",
    "        \n",
    "        # log determinant of N\n",
    "        logdet_N = np.sum(np.log(Nvec))\n",
    "\n",
    "        # triple product in likelihood function\n",
    "        rNr = np.sum(yred**2/Nvec)\n",
    "\n",
    "        # first component of likelihood function\n",
    "        loglike = -0.5 * (logdet_N + rNr)\n",
    "            \n",
    "        return loglike   \n",
    "    \n",
    "\n",
    "    # this can and should be much cleaner\n",
    "    def get_lnlikelihood(self, xs):\n",
    "        \n",
    "        # map parameter vector\n",
    "        params = self.map_params(xs)\n",
    "        \n",
    "        # start likelihood calculations\n",
    "        loglike = 0\n",
    "        \n",
    "        # get auxiliaries\n",
    "        Nvec = self._alpha**self._z * self.pta.get_ndiag(params)[0]\n",
    "        phiinv, logdet_phi = self.pta.get_phiinv(params, logdet=True)[0]\n",
    "        residuals = self._residuals\n",
    "        \n",
    "        T = self.pta.get_basis(params)[0]\n",
    "        if self.TNT is None and self.d is None:\n",
    "            self.TNT = np.dot(T.T, T / Nvec[:,None])\n",
    "            self.d = np.dot(T.T, residuals/Nvec)\n",
    "        #d = self.pta.get_TNr(params)[0]\n",
    "        #TNT = self.pta.get_TNT(params)[0]\n",
    "\n",
    "        # log determinant of N\n",
    "        logdet_N = np.sum(np.log(Nvec))\n",
    "\n",
    "        # triple product in likelihood function\n",
    "        rNr = np.sum(residuals**2/Nvec)\n",
    "\n",
    "        # first component of likelihood function\n",
    "        loglike += -0.5 * (logdet_N + rNr)\n",
    "\n",
    "        # Red noise piece\n",
    "        Sigma = self.TNT + np.diag(phiinv)\n",
    "        \n",
    "        try:\n",
    "            cf = sl.cho_factor(Sigma)\n",
    "            expval = sl.cho_solve(cf, self.d)\n",
    "        except np.linalg.LinAlgError:\n",
    "            return -np.inf \n",
    "        \n",
    "        logdet_sigma = np.sum(2 * np.log(np.diag(cf[0])))\n",
    "        loglike += 0.5 * (np.dot(self.d, expval) - logdet_sigma - logdet_phi)\n",
    "        \n",
    "        return loglike\n",
    "    \n",
    "    def get_lnlikelihood_df(self, df):\n",
    "        n = len(self._residuals)\n",
    "        ll = -(df/2) * np.sum(np.log(self._alpha)+1/self._alpha) + \\\n",
    "            n * (df/2) * np.log(df/2) - n*scipy.special.gammaln(df/2)\n",
    "        return ll    \n",
    "    \n",
    "    def get_lnprior(self, xs):\n",
    "        \n",
    "        return np.sum(p.get_logpdf(x) for p, x in zip(self.params, xs))\n",
    "    \n",
    "    \n",
    "    def sample(self, xs, niter=10000):\n",
    "        \n",
    "        self.chain = np.zeros((niter, len(xs)))\n",
    "        self.bchain = np.zeros((niter, len(self._b)))\n",
    "        self.thetachain = np.zeros(niter)\n",
    "        self.zchain = np.zeros((niter, len(self._residuals)))\n",
    "        self.alphachain = np.zeros((niter, len(self._residuals))) \n",
    "        self.poutchain = np.zeros((niter, len(self._residuals)))\n",
    "        self.dfchain = np.zeros(niter)\n",
    "        \n",
    "        xnew = xs\n",
    "        tstart = time.time()\n",
    "        for ii in range(niter):\n",
    "            self.chain[ii, :] = xnew\n",
    "            self.bchain[ii,:] = self._b\n",
    "            self.zchain[ii,:] = self._z\n",
    "            self.thetachain[ii] = self._theta\n",
    "            self.alphachain[ii,:] = self._alpha\n",
    "            self.dfchain[ii] = self.tdf\n",
    "            self.poutchain[ii, :] = self._pout\n",
    "            \n",
    "            self.TNT = None\n",
    "            self.d = None\n",
    "\n",
    "            # update white parameters\n",
    "            xnew = self.update_white_params(xnew)\n",
    "            \n",
    "            # update hyper-parameters\n",
    "            xnew = self.update_hyper_params(xnew)\n",
    "            \n",
    "            # if accepted update quadratic params\n",
    "            if np.all(xnew != self.chain[ii,-1]):\n",
    "                self._b = self.update_b(xnew)\n",
    "            \n",
    "            # update outlier model params\n",
    "            self._theta = self.update_theta(xnew)\n",
    "            self._z = self.update_z(xnew)\n",
    "            self._alpha = self.update_alpha(xnew)\n",
    "            self.tdf = self.update_df(xnew)\n",
    "            \n",
    "            if ii % 100 == 0 and ii > 0:\n",
    "                sys.stdout.write('\\r')\n",
    "                sys.stdout.write('Finished %g percent in %g seconds.'%(ii / niter * 100, time.time()-tstart))\n",
    "                sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = '/Users/jaellis/Work/pulsar_data/nanograv/nanograv_5yr/'\n",
    "parfile = datadir + 'par/J1643-1224_NANOGrav_dfg+12.par'\n",
    "timfile = datadir + 'tim/J1643-1224_NANOGrav_dfg+12.tim'\n",
    "\n",
    "#parfile = '/Users/jaellis/Work/pulsar_data/nanograv/nanograv_timing_2013/release/par/B1937+21_NANOGrav_9yv1.gls.par'\n",
    "#timfile = '/Users/jaellis/Work/pulsar_data/nanograv/nanograv_timing_2013/release/tim/B1937+21_NANOGrav_9yv1.tim'\n",
    "\n",
    "#parfile = 'simulated_data/outliers/J1713+0747.par'\n",
    "#timfile = 'simulated_data/outliers/J1713+0747.tim'\n",
    "\n",
    "#parfile = 'simulated_data/gaussian/J1713+0747.par'\n",
    "#timfile = 'simulated_data/gaussian/J1713+0747.tim'\n",
    "\n",
    "#parfile = 'simulated_data/studentt/J1713+0747.par'\n",
    "#timfile = 'simulated_data/studentt/J1713+0747.tim'\n",
    "\n",
    "psr = Pulsar(parfile, timfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6686f2185152>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mquantiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.99\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "quantiles = np.linspace(0.1, 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# white noise\n",
    "efac = parameter.Uniform(0.5, 10)\n",
    "#efac = parameter.Constant(1.0)\n",
    "equad = parameter.Uniform(-10, -5)\n",
    "ecorr = parameter.Uniform(-10, -5)\n",
    "\n",
    "# backend selection\n",
    "#selection = selections.Selection(selections.by_backend)\n",
    "selection = selections.Selection(selections.no_selection)\n",
    "\n",
    "ef = white_signals.MeasurementNoise(efac=efac, selection=selection)\n",
    "eq = white_signals.EquadNoise(log10_equad=equad, selection=selection)\n",
    "ec = gp_signals.EcorrBasisModel(log10_ecorr=ecorr, selection=selection)\n",
    "\n",
    "# red noise\n",
    "pl = utils.powerlaw(log10_A=parameter.Uniform(-18,-12), gamma=parameter.Uniform(1,7))\n",
    "rn = gp_signals.FourierBasisGP(spectrum=pl, components=30)\n",
    "\n",
    "# timing model\n",
    "tm = gp_signals.TimingModel()\n",
    "\n",
    "# combined signal\n",
    "s = ef + eq  + rn + tm + ec\n",
    "              \n",
    "# PTA\n",
    "pta = signal_base.PTA([s(psr)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pta.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P0 = 1/218.8118405230054218\n",
    "smin = np.max(psr.toaerrs)\n",
    "alpha = P0**2/smin**2 \n",
    "gs = lambda x: 1/np.sqrt(2*np.pi*alpha*smin**2) * np.exp(-x**2/2/alpha/smin**2)\n",
    "x = np.linspace(psr.residuals.min(), psr.residuals.max(), 1000)\n",
    "plt.plot(x, gs(x)/gs(x).max() - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#mdls = []\n",
    "labels = ['vvh17', 'individual', 'uniform', 'beta', 'gaussian', 't']\n",
    "# emulate Vallisneri and van Haasteren mixture model\n",
    "# what should alpha be?\n",
    "gibbs = Gibbs2(pta, model='vvh17', vary_df=False, theta_prior='uniform', \n",
    "               vary_alpha=False, alpha=1e10, pspin=0.00457)\n",
    "mdls.append(gibbs)\n",
    "\n",
    "#gibbs = Gibbs(pta, model='mixture')\n",
    "#mdls.append(gibbs)\n",
    "\n",
    "# uniform theta distribution\n",
    "gibbs = Gibbs2(pta, model='mixture', vary_df=True, theta_prior='uniform')\n",
    "mdls.append(gibbs)\n",
    "\n",
    "# beta theta distribution\n",
    "gibbs = Gibbs2(pta, model='mixture', vary_df=True, theta_prior='beta')\n",
    "mdls.append(gibbs)\n",
    "\n",
    "# Gaussian\n",
    "gibbs = Gibbs2(pta, model='gaussian', vary_df=True, theta_prior='beta')\n",
    "mdls.append(gibbs)\n",
    "\n",
    "# t-distribution\n",
    "gibbs = Gibbs2(pta, model='t', vary_df=True, theta_prior='beta')\n",
    "mdls.append(gibbs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for md in mdls[]:\n",
    "md = mdls[5]\n",
    "params = np.array([p.sample() for p in md.params]).flatten()\n",
    "niter = 10000\n",
    "md.sample(params, niter=niter)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mpars = np.load('/Users/jaellis/Downloads/J1643-1224-pars.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pmap = {'1643-1224_efac': 0, '1643-1224_gamma': 4, \n",
    "        '1643-1224_log10_A': 3, '1643-1224_log10_ecorr': 2, \n",
    "        '1643-1224_log10_equad': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['vvh17', 'uniform', 'beta', 'gaussian', 't']\n",
    "labels = ['vvh17', 'vvh172']\n",
    "plt.figure(figsize=(16,9))\n",
    "mdls2 = [mdls[0], mdls[5]]\n",
    "for md, label in zip(mdls2, labels):\n",
    "    pars = sorted([par.name for par in md.params])\n",
    "    out = np.flatnonzero(md.zchain.mean(axis=0)>0.90)\n",
    "    print label, len(out)\n",
    "    for ii in range(5):\n",
    "        plt.subplot(3,2,ii+1)\n",
    "        plt.hist(md.chain[300:,ii], bins='auto', normed=True, histtype='step', lw=2, label=label)\n",
    "        plt.xlabel(pars[ii])\n",
    "for ii in range(5):\n",
    "    plt.subplot(3,2,ii+1)\n",
    "    plt.hist(mpars[:,pmap[pars[ii]]], bins='auto', normed=True, histtype='step', lw=2, label='m')\n",
    "plt.tight_layout()\n",
    "plt.legend(loc='best', fontsize=12)\n",
    "\n",
    "#plt.savefig('hyper_pars_t.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels = ['vvh17', 'uniform', 'beta', 'gaussian', 't']\n",
    "plt.figure(figsize=(15,3*37))\n",
    "for md, label in zip(mdls, labels):\n",
    "    pars = sorted([par.name for par in md.params])\n",
    "    out = np.flatnonzero(md.zchain.mean(axis=0)>0.90)\n",
    "    print label, len(out)\n",
    "    for ii in range(54):\n",
    "        plt.subplot(27,2,ii+1)\n",
    "        plt.hist(md.bchain[300:,ii], bins='auto', normed=True, histtype='step', lw=2, label=label)\n",
    "        #plt.title(pars[ii])\n",
    "plt.tight_layout()\n",
    "plt.savefig('b_pars_t.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,3*6))\n",
    "ct = 0\n",
    "for md, label in zip(mdls, labels):\n",
    "    plt.subplot(5,1,ct+1)\n",
    "    Tmat = pta.get_basis()[0]\n",
    "    out = np.flatnonzero(md.zchain.mean(axis=0)>0.90)\n",
    "    for ii in range(500):\n",
    "        ind = np.random.randint(100, 10000)\n",
    "        plt.plot(psr.toas/86400, np.dot(Tmat, md.bchain[ind, :])*1e6, color='C0', alpha=0.2)\n",
    "    #plt.errorbar(psr.toas/86400, psr.residuals*1e6, psr.toaerrs*1e6, fmt='.', color='k')\n",
    "    if len(out) < len(psr.toas):\n",
    "        plt.errorbar(psr.toas[out]/86400, psr.residuals[out]*1e6, psr.toaerrs[out]*1e6, fmt='o', color='C1')\n",
    "    plt.title(label, fontsize=16)\n",
    "    ct += 1\n",
    "plt.tight_layout()\n",
    "#plt.savefig('waveform_t.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = np.flatnonzero(mdls[1].zchain.mean(axis=0)>0.9)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for o in out:\n",
    "    print np.median(mdls[1].poutchain[500:,o])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, big_axes = plt.subplots( figsize=(15.0, 3*6) , nrows=2, ncols=1, sharey=False) \n",
    "labels = ['vvh17', 'vvh172']\n",
    "for row, big_ax in enumerate(big_axes, start=1):\n",
    "    big_ax.set_title(labels[row-1], fontsize=16)\n",
    "\n",
    "    # Turn off axis lines and ticks of the big subplot \n",
    "    # obs alpha is 0 in RGBA string!\n",
    "    big_ax.tick_params(labelcolor=(1.,1.,1., 0.0), top='off', bottom='off', left='off', right='off')\n",
    "    # removes the white frame\n",
    "    big_ax._frameon = False\n",
    "    \n",
    "ct = 0\n",
    "for md, label in zip(mdls, labels):\n",
    "    ax = fig.add_subplot(5,4,4*ct+1)\n",
    "    out = np.flatnonzero(md.zchain.mean(axis=0)>0.90)\n",
    "    plt.plot(psr.toas/86400, md.alphachain[1000:,:].mean(axis=0), '.')\n",
    "    plt.plot(psr.toas[out]/86400, md.alphachain[1000:,out].mean(axis=0), '.')\n",
    "    plt.ylabel(r'Scale mixture parameter [$\\overline{\\alpha}_j$]')\n",
    "    plt.xlabel(r'MJD')\n",
    "    \n",
    "    ax = fig.add_subplot(5,4,4*ct+2)\n",
    "    plt.plot(psr.toas/86400, md.zchain.mean(axis=0), '.')\n",
    "    #plt.plot(psr.toas/86400, np.median(md.poutchain, axis=0), '.')\n",
    "    plt.plot(psr.toas[out]/86400, md.zchain.mean(axis=0)[out], '.')\n",
    "    plt.ylabel(r'Outlier indicator [$\\overline{z}_j$]')\n",
    "    plt.xlabel(r'MJD')\n",
    "    \n",
    "    if label in ['vvh17', 'uniform', 'beta']:\n",
    "        ax = fig.add_subplot(5,4,4*ct+3)\n",
    "        plt.hist(md.thetachain[1000:], bins='auto', normed=True);\n",
    "        if label == 'beta':\n",
    "            k = len(psr.residuals)\n",
    "            m = 0.01\n",
    "            y = scipy.stats.beta(k*m, k*(1-m))\n",
    "            x = np.linspace(0.0, md.thetachain[1000:].max(), 1000)\n",
    "            plt.plot(x, y.pdf(x), lw=2, color='C2')\n",
    "        else:\n",
    "            plt.axhline(1, lw=2, color='C2')\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "        plt.xlabel(r'Outlier Probability [$\\theta$]')\n",
    "        if label == 'vvh17':\n",
    "            plt.hist(mpars[:,5], bins='auto', normed=True, histtype='step', lw=2, color='C1')\n",
    "     \n",
    "    if label in ['uniform', 'beta', 't']:\n",
    "        ax = fig.add_subplot(5,4,4*ct+4)\n",
    "        dfs = np.unique(md.dfchain[1000:])\n",
    "        counts = np.array([sum(md.dfchain[1000:]==df) for df in dfs])\n",
    "        plt.bar(dfs, counts, align='center')\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "        plt.xlabel('t-disribution d.o.f')\n",
    "    ct += 1\n",
    "plt.tight_layout()\n",
    "#plt.savefig('outlier_pars_t.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.plot(mdls[1].thetachain[100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(mdls[0].thetachain[100:], 50, normed=True);\n",
    "plt.hist(mdls[5].thetachain[100:], 50, normed=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
